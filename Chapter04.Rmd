---
title: "Properties of Estimators and Confidence Intervals"
author: "László Kovács"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify;
font-size: 12pt}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Simulating Sampling Distributions

Okay, in the beginning of this chapter, it's time to play God! Let's assume that we have a $Y$ variable that has a true population distribution of $Exp (0.01)$. So, we are Gods now, as we don't try to guess from observed data what kind of theoretical distribution best describes the unobserved population, but we explicitly create a $Y$ variable, where we know that the true distribution is $Exp(0.01)$. So, basically we have $$Y \sim Exp(0.01)$$

Based how an exponential distribution behaves, we also know the true population expected value ($\mu$) and true population standard deviation ($\sigma$) for the variable $Y$. So we have that

- $E(Y)=\mu= 1/0.01=100$
- $D(Y)=\sigma= 1/0.01=100$

Let's save all these information into 3 R objects!

```{r}
rate <- 0.01
mu <- 1/rate
sigma <- 1/rate
```

Then, let's do an IID **resampling** of this $Exp(0.01)$ distribution. Let's have $10000$ IID samples of size $n=120$. The technical solutions in R for this resampling are done exactly the same way we saw in <a href="Chapter02.html" target="_blank">Section 4.1 of Chapter 2</a>. So, in the end we'll have a data frame called `exp_samples`, where we have the $10000$ samples stored in the rows and the $120$ observations are stored in the columns. So it'll be a data frame of $10000 \times 120$.<br>
Let's fix the random seed on $1992$ for reproducability.

```{r}
sample_size <- 120
replication_number <- 10000

set.seed(1992)
exp_samples <- replicate(replication_number, rexp(n = sample_size, rate = rate))
exp_samples <- as.data.frame(t(exp_samples))

rownames(exp_samples) <- paste0("Sample",1:replication_number)
colnames(exp_samples) <- paste0("Element",1:sample_size)
head(exp_samples)
```

Great, we have the data frame we want! Now, let's calculate the sample means and standard deviations with the built-in R function for each of the $10000$ samples.<br>
Using the `mean` and `sd` functions within an `apply` function with the 2nd input parameter of `1`, we compute the sample means, and standard deviations row-wise instead of column-wise. **Be careful** that since the data frame columns are continuously expanding, we must **manually restrict the application of statistical functions to the first 120 columns**!

```{r}
exp_samples$sample_mean <- apply(exp_samples[,1:sample_size], 1, mean)
exp_samples$sample_sd <- apply(exp_samples[,1:sample_size], 1, sd)
head(exp_samples[,(sample_size-1):ncol(exp_samples)]) # look at the last 4 columns
```

Great, so in the 1st sample the mean is $82.9$ and standard deviation is $89.7$. While in the 6th sample, the mean is $92.6$ and standard deviation is $92.4$.<br>
These sample means ($\bar{y}$) are called as **ESTIMATORS** for the true expected value of $\mu=100$. In the same logic, the sample standard deviations ($s$) are **estimators** for the true standard deviation of $\sigma=100$.<br>
In the same logic, the $\hat{\lambda}_{ML}=1/\bar{y}$ maximum likelihood estimation derived for $\lambda$ in <a href="Chapter03.html" target="_blank">Section 2.1 of Chapter 3</a> is the **estimator** for the true $\lambda$ for our exponential distribution.

```{r}
exp_samples$sample_rate <- 1 / exp_samples$sample_mean
head(exp_samples[,(sample_size-1):ncol(exp_samples)])
```

So in the 3rd sample, we would say that our distribution is $Exp(0.0095)$, but in the 5th one we would guess an $Exp(0.0117)$ distribution.

Collectively, we denote every estimator as $\hat{\theta}=\{\bar{y},s,\hat{\lambda}_{ML}\}$, and we say that these are **estimators of the true** population distribution **parameters** denoted as $\theta=\{\mu,\sigma,\lambda\}$.

And like we saw from our simulations, these estimators are not the same for our 10000 samples due to sampling error as we've seen in <a href="Chapter02.html" target="_blank">Chapter 2</a>. So these estimators have distributions along the lots of IID samples and this **distribution of estimators along many samples are called as sampling distributions**.

For example, if we show our 10000 simulated sample means on a histogram, what we see is the sampling distribution of the means. Let's add the true expected value as a red vertical line to the histogram with the help of the `geom_vline` layer in `ggplot`.

```{r}
library(ggplot2)
ggplot(exp_samples, aes(x=sample_mean)) + geom_histogram(aes(y = after_stat(density))) +
  geom_vline(xintercept = mu, color="red", linewidth=1)
```

We can see that it looks like a nice normal distribution with the true $\mu=100$ at the center. So, we can say that the sample means are fluctuating around the true expected value and the proportion of under- and overestimations is equal. Also, estimators being far off (like over a threshold of roughly $\pm 20$) from the true parameter of $\mu=100$ is rather a rare event.

The same things can be said for the sampling distribution of $\hat{\lambda}_{ML}$ estimators. It's not surprising perhaps as this derived as the reciprocal of sample means.

```{r}
ggplot(exp_samples, aes(x=sample_rate)) + geom_histogram(aes(y = after_stat(density))) +
  geom_vline(xintercept = rate, color="red", linewidth=1)
```

Nice plot! We have one pesky little overestimation with one $\hat{\lambda}_{ML} > 0.014$, buit otherwise, we can see rather nice normal bell curve here as well for the sampling distribution.

Now, let's take a look at the sampling distribution of the sample standard deviations ($s$) with true standard deviation of $\sigma=100$ added as a red vertical line here too.

```{r}
ggplot(exp_samples, aes(x=sample_sd)) + geom_histogram(aes(y = after_stat(density))) +
  geom_vline(xintercept = sigma, color="red", linewidth=1)
```

In this case, the sampling distribution deviates a bit more from the normal distribution. It seems that it has a mild right tail: underestimations are a bit more common than overestimations, and we have a bit more outliers on the right tail of the distributions. That is *we are usually underestimating, but in the few times when we overestimate, we overestimate by a lot*!

At this point it's important to note that the $s$ estimator for $\sigma$ calculated by R's `sd` function is the **corrected sample standard deviation formula** that divides by $n-1$ in the end: $$s=\sqrt{\frac{\sum_{i=1}^n{(y_i-\bar{y})^2}}{n-1}}$$

However, we could also try the original, **uncorrected sample standard deviation formula**, denoted as $s^*$, as an estimator for the true $\sigma$. This $s^*$ estimator only differs from $s$ that it divides by $n$ in the end, so it is the actual expected deviation from the mean: $$s^*=\sqrt{\frac{\sum_{i=1}^n{(y_i-\bar{y})^2}}{n}}$$

Due to these definitions, the connection between the two estimators is the following: $$s=\sqrt{\frac{n}{n-1}} \times s^*$$

From this, we can see that $s^*$ is multiplied with a number larger than $1$ to get $s$, so **the corrected sample standard deviation is always bigger than the uncorrected one**.

We should also remember from <a href="Chapter03.html" target="_blank">Section 3.1 of Chapter 3</a>, that for normal distribution $s^*$ is the maximum likelihood estimator for the true population standard deviation $\sigma$: $s^*=\hat{\sigma}_{ML}$.

So, let's create a custom R function for this original standard deviation formula!

```{r}
original_sd <- function(x) sqrt(mean((x-mean(x))^2))
```

Then, using this function combined with `apply`, let's calculate the $s^*$ estimators for $\sigma$ for all the 10000 samples as well.

```{r}
exp_samples$orig_sd <- apply(exp_samples[,1:sample_size], 1, original_sd)
head(exp_samples[,(sample_size-1):ncol(exp_samples)])
```

Note from the results that what we expected based on the formulas is true: `sample_sd > orig_sd`, or with our formal notations: $s > s^*$. However the difference is small since $n=120$ is quite large, that means $\sqrt{n/(n-1)} \approx 1$.

So, **with $\hat{\theta}$ now we have two estimators for the same $\theta=\sigma$ parameter**! So, let's investigate the **sampling distribution of our 2nd estimator**, $s^*$!

```{r}
ggplot(exp_samples, aes(x=orig_sd)) + geom_histogram(aes(y = after_stat(density))) +
  geom_vline(xintercept = sigma, color="red", linewidth=1)
```

Ok, we see a very similar distribution than the one of $s$ as the difference between the two estimators is not large as $\sqrt{n/(n-1)} \approx 1$ due to the large $n$ sample size. However, if we overlay the two sampling distributions on each other, we can notice that the **underestimations are more common for the uncorrected version than for the corrected one**.

```{r}
ggplot(exp_samples) +
  geom_histogram(aes(x=sample_sd, y = after_stat(density), fill="s"), alpha=0.3) +
  geom_histogram(aes(x=orig_sd, y = after_stat(density), fill="s*"), alpha=0.3) +
  geom_vline(xintercept = sigma, color="red", linewidth=1)
```


So, at the end of this section, hopefully we can see how useful these **sampling distributions** can be: they **show how accurate estimators are compared to the true parameter value in case of lots of samples**. They show

- how close most of the estimators are to true parameter
- the proportions of under- and overestimations
- the frequency, magnitude and direction of outlier estimates

> Therefore, knowing sampling distributions are useful because they can describe from several perspectives how accurately our estimators from observed data can capture the true population parameters.

So, we should **find ways to determine the sampling distribution of estimators based on only one observed sample and not based on a simulations**. As in real-world applications we can **only observe one sample and that's it**, as we have seen in the practical examples of of Chapters 1-3 so far. Since in reality, we are not Gods unfortunately, and we don't know the true population distribution of our observed data. That is the point of this whole show. :)

> Be very careful here, and notice that the sampling distribution is NOT the sample as the true distribution of the unobserved population!!<br>
> The sampling distribution is always about how an estimator behaves in many samples.<br>
> The population distribution describes how unobserved individual datapoints of our variable (be it age, income, education, whatever) are behaving.

So, in the following sections, we'll see some **theorems** and rules that **give the theoretical sampling distribution of some special estimators if the data satisfies certain conditions** (like large sample size).

But first, in the next section we examine some general concepts that are tied to the **first two moments of sampling distributions**, so to the **expected value and variance** (squared standard deviation) **of estimators**.

## 2. Bias and (Estimation) Variance

In this section, we investigate the first two moments of sampling distributions in detail. Namely, we introduce two concepts tied to the **first two moments of the sampling distribution of $\hat{\theta}$ estimators**.

1. **Bias** is tied to the expected value of estimators: $M_1(\hat{\theta})=E(\hat{\theta})$
2. **Estimation Variance** is the second moment of estimators: $M_2(\hat{\theta})=Var(\hat{\theta})$

Let's go ahead with the first moment!

### 2.1. Bias

So, the concept of bias concerns the **expected value, or** in other word, the **mean of the estimators**: $E(\hat{\theta})$

Let's calculate this expected value or mean for all our estimators from Section 1.

```{r}
c(mean(exp_samples$sample_mean),
  mean(exp_samples$sample_sd),
  mean(exp_samples$orig_sd),
  mean(exp_samples$sample_rate))
```

We can see that these expected values are close to the true parameter values: $\mu=\sigma=100,\lambda=0.01$. Bias simply examines how close we are. So, the **definition of Bias** is the following: $$Bs=E(\hat{\theta})-\theta$$

Essentially, Bias asks that the question **whether the $\hat{\theta}$ estimator on average under- or overestimates the true $\theta$ parameter**. If on average we hit the true parameter ($E(\hat{\theta})=\theta$), we say that the estimator is **unbiased** as it does not give systematically higher or lower estimates. We have the same chances of mistakes on either side.

So, let's see the **Bias of the means** rounded to 5 digits.

```{r}
round(mean(exp_samples$sample_mean) - mu,5)
```

Looks like it's quite small, very close to $0$. Actually, **theoretically the mean is always an unbiased estimator for the true expected value**. The Bias is not exactly $0$ here as we **do not have every possible IID sample of size 120** from our $Exp(0.01)$ distribution, **"only" 10000 of them**. So, we have a very small amount of bias, which is negligable.<br>
Furthermore it is very easy to see that the sample mean is an unbiased estimator. We need to show for $\theta=\mu=E(Y)$ and $\hat{\theta}=\bar{y}=\sum_{i=1}^n{y_i}/n$ that $E(\hat{\theta})=\theta$.

We now that we have an IID sample which means that every $y_i$ observation has the same exponential distribution now: $y_i \sim Exp(0.01)$. That also means that if we treat every $y_i$ observation as a random variable, then their expected values are the same: $$E(y_i)=\mu=\frac{1}{\lambda}=\frac{1}{0.01}=100$$

So, if we take the expected value of the sample mean, we have: $$E(\bar{y})=E\left(\frac{\sum_{i=1}^n{y_i}}{n}\right)=\frac{1}{n}E\left(\sum_{i=1}^n{y_i}\right)=\frac{1}{n}\sum_{i=1}^n{E(y_i)}=\frac{1}{n}\sum_{i=1}^n{\mu}=\frac{1}{n} \times n \times \mu = \mu$$

We only applied two properties of the expected value for the proof above. Notice that we did not need to take advantage of the fact what is the exact distribution of our $Y$ variable. So, this result is true for every mean and every expected value. No assumption is necessary.

1. If $c$ is a constant and $Y$ is a random variable, then $E(cY)=cE(Y)$
2. If $X,Y$ are random variables, then $E(X+Y)=E(X)+E(Y)$

And in the end we have that the expected value of the $\hat{\theta}=\bar{y}$ estimator is the same as the population parameter $\theta=\mu$

Now let's see the biases of the two estimators $\hat{\theta}=\{s,s^*\}$ for the $\sigma$ parameter. Let's also round the results for 5 digits!

```{r}
c(round(mean(exp_samples$sample_sd) - sigma,5),
  round(mean(exp_samples$orig_sd) - sigma,5))
```

What we can observe is that the corrected version has a smaller bias, but both estimators have a slight downward bias: e.g. underestimations are a bit more common than overestimations. This is exactly what we could observe on the histograms of both sampling distributions in Section 1.

The theoretical results here are a bit more complicated to prove here than in the case of the mean, but for the variances we have a result here that the uncorrected variance has a theoretical downward bias: $$E((s^*)^2)=\frac{n-1}{n}\sigma^2$$

And that is why the corrected version is defined as we've seen in Section 1: $$s^2=\frac{n}{n-1}(s^*)^2$$

Since this way the **corrected version is unbiased estimator for the true variance** $\sigma^2$: $$E(s^2)=\sigma^2$$

However, in the simulation a **small** amount of **bias remains** due to the fact that **again, we do not have every possible IID sample of size 120** from our $Exp(0.01)$ distribution, **"only" 10000 of them**. And **for the standard deviation the square root further distorts the remaining small bias a bit in our simulations**. Because of how sqare root works for number between 0 and 1. E.g. $\sqrt{0.01}=0.1$

Also, it is also important to know that the **uncorrected standard deviation is an asymptotically unbiased estimator**. This means that if the **sample size increases** (it goes to infinity) **, then the bias disappears**: $$\lim_{n\to\infty}{Bs(s^*)}=0$$

This is true because we know that $$E((s^*)^2)=\frac{n-1}{n}\sigma^2$$

And of course it is elementary calculus that the fraction before $\sigma^2$ goes to $1$ if $n \to \infty$: $$\lim_{n\to\infty}{\frac{n-1}{n}}=1$$

Definition of Standard Error: $$SE=\sqrt{Var\left(\hat\theta\right)}=\sqrt{E\left(\left(\hat\theta-E\left(\hat{\theta}\right)\right)^2\right)}$$

### 2.2. Estimation Variance and Standard Error

It’s nice and all that we established that the sample mean, and corrected sample variance + standard deviation are **unbiased estimators** of their corresponding population parameters, $\theta$, but **what good is this to us in practice when we only have a single sample?**

As we experienced, unbiasedness only means that if **we have a very, very large number of samples**, then the estimators **on average hit the true population value**. However, unfortunately, **this definition allows for a lot of variation**, and in reality, it **tells us nothing about the sampling error**!<br>
This problem is well illustrated by the following *bad joke*.

> "An engineer, a physicist, and a statistician go hunting for wild boars together. After taking just a few steps into the forest, they spot a massive boar 150 meters away.<br>
>  The engineer raises his rifle, aims, and shoots, but misses by three meters to the right. The physicist thinks:<br>
>  "A slight wind is blowing from the left, so if I aim a little to the left, I’ll hit it."<br>
>  He takes aim at the boar, fires, and misses by three meters to the left. The statistician jumps up and starts celebrating:<br>
>  "We got it! We got it! We hit it!"
>
> `r tufte::quote_footer('-- Rightfully Unknown Author')`

Our beloved statistician in the joke is celebrating because the two shots that missed by three meters to the right and left, on average, hit the poor boar dead center!<br>
Well, this is what **unbiasedness** is all about:

- We want to hunt down a true population parameter ($\theta$) using *shots*, that is, *samples*.
- The first shot is the estimate $\hat{\theta}_1$ from the first sample
- The second shot is the estimate $\hat{\theta}_2$ from the second sample.
- On average, that is, in expected value, these estimates hit the true value $\theta$: $E(\hat{\theta})=\theta$.
- Voilà! This is the definition of an *unbiased estimate* :)

The **analogy** between statistical "$\hat{\theta}$"-hunting and wild boar hunting can be **illustrated in the following diagram**. **IMPORTANT** :)

<center>
![](BiasBoar.jpg){width=50%}
</center>

<br>In the above diagram, I have marked in **blue** the **distance between an arbitrary $\hat{\theta}_i$ and the true population parameter $\theta$.** In reality, **this is the distance we are interested in: the difference between an estimator from a given IID sample and the true population parameter**! Because in practice, **we only have a single sample, and we need to compute the distance between the one observed $\hat{\theta}$ and the true $\theta$!**

Now, **thanks to unbiasedness**, we have a method to **calculate this distance**! Since, due to unbiasedness, $E(\hat{\theta})=\theta$, this means that the **standard deviation of the estimators $\hat{\theta}_i$ across many samples is precisely the blue distance we are seeking**. After all, what is the **general interpretation of standard deviation**? It tells us **how much a randomly selected value from a dataset is expected to deviate from the mean**. How does this translate to the set of $\hat{\theta}$ values? If we **randomly select one of the many possible samples, the estimator $\hat{\theta}$ from that sample is expected to deviate from the mean of the $\hat{\theta}$s by the standard deviation**. And the **mean of the $\hat{\theta}$s, due to unbiasedness, is precisely the true population value $\theta$.**<br>
From this reasoning, the **standard deviation of the $\hat{\theta}$ values is called the standard error of estimation**, or simply **standard error** (**SE**).

Notice that this is practically the **second moment of the sampling distribution**, the square root of the variance for our $\hat{\theta}$ estimators. $$SE=\sqrt{Var(\hat{\theta})}$$

Since variance of a random variable $Y$ is defined as $E((Y-E(Y))^2)$ (and standard deviation is the square root of this), which is the expected (average) squared deviation from the expected value (mean) we need to apply our `original_sd` function to get the standard errors: no need for correction here, we calculate standard deviation of the estimators the *original way*.

So, using this knowledge, let's have the Standard Error of the means.

```{r}
original_sd(exp_samples$sample_mean)
```

We can get the same Standard Error of the corrected standard deviations.

```{r}
original_sd(exp_samples$sample_sd)
```


Since the **sample mean and corrected standard deviation are unbiased estimators**, the **computed standard error can be interpreted as follows**:

- For 100-element samples, **a specific sample mean is expected to deviate by $9.088$ from the mean of means, that is, from the true population mean of 100**.
- **In a specific 100-element sample, the standard  deviation is expected to deviate by $12.589$ from the true standard deviation** that is also 100 due to the exponential distribution of the data.

We can also get the **Standard Error of uncorrected sample standard deviation**s. But this **cannot be interpreted as nicely** as the two $SE$s from before since it is a **biased estimator** for $\sigma$.

```{r}
original_sd(exp_samples$orig_sd)
```

We can only say here that For 100-element samples, a specific uncorrected sample  standard deviation is expected to deviate by $12.537$ from the mean of these uncorrected standard deviations and that's it. Since the mean of these uncorrected standard deviations is NOT the true $\sigma$ parameter.

For the **sample means**, we have a nice **closed formula** on how to express the **standard error**.

The formula follows from the fact that if I have an IID sample, then the sample observations $y_i$ are considered as random variables with  the same standard deviation of $\sigma$ (for exponential this is always $1/\lambda$ just like we saw for $\mu$ in Section 2.1), then their sum, $\sum_{i=1}^n{y_i}$ has a variance of $$Var(\sum_{i=1}^n{y_i})=\sum_{i=1}^n{Var(y_i)}=n \times \sigma^2$$

**Because the $y_i$ random variables are independent due to the random sampling**, so we don't need to account for $cov(y_i,y_j)$ values. Since the sample means are defined as $\bar{y}=\frac{\sum_{i=1}^n{y_i}}{n}$, their variance follows from the result obtained above: $$Var(\bar{y})=\frac{Var(\sum_{i=1}^n{y_i})}{n^2}=\frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$$

And of course the **square root of the sample mean's variance is** the standard deviation of the sample means, so the **standard error of the mean**: $$\sqrt{Var(\bar{y})}=SE(\bar{y})=\frac{\sigma}{\sqrt{n}}$$

Thus, **standard error of the mean can be determined if we know the population standard deviation ($\sigma$) and the sample size ($n$).** Let's test this out!

```{r}
c(original_sd(exp_samples$sample_mean),
  sigma/sqrt(sample_size))
```

It appears that the **two values match quite closely**. :) The slight discrepancy is again due to the fact that we did not examine all possible samples, only 10000 when calculating the $SE$ as the standard deviation of $\hat{\theta}$ values.

Unfortunately, **we do NOT have similar approximation formulas for standard deviations and other estimators like medians**. We will try other tricks to estimate $SE$ for those later in the course. :)

One last important **naming convention: The squared SE value ($SE^2$) is often called the variance of the estimator**. I don’t personally like this term, as it’s easy to confuse it with the variance of the sample or population observations. However, it is widely used, so it's important to know! If you **see something like "the variance of sample means is A" or "the variance of sample proportions is B," the author is referring to $SE^2$.**
This is typically denoted as follows: $SE^2(\hat{\theta})=Var(\hat{\theta})$

To summarize, here are the **different types of standard deviations and variances we have encountered in estimation theory**:

- **Population Standard Deviation**: Expected difference of an observation in the population from the population mean. Denoted as: $\sigma$.
  * Squared: population variance, $\sigma^2$
- **Uncorrected Sample Standard Deviation**: Expected difference of an observation in the sample from the sample mean. Denoted as: $s^*$
  * Squared: uncorrected sample variance, $(s^*)^2$
- **Corrected Sample Standard Deviation**: Same thing as the Uncorrected Sample Standard Deviation, it just provides an *unbiased* estimate for teh true population standard deviation. Denoted as: $s$
  * Squared: corrected sample variance, $s^2$
- **Standard error**: The standard deviation of an estimator, i.e., $\hat{\theta}$, calculated from many-many samples. *In the case of unbiasedness*, the difference of an estimator computed from a single sample from the true population value of the $\theta$ parameter. Denoted as: $SE(\hat{\theta})$
  * Squared: variance of the estimator, $Var(\hat{\theta})$

The formula for the $SE$ of the mean also implies that, the **sample mean** is a so-called **consistent estimator for the true expected value**.

In general, an **estimator $\hat{\theta}$ is consistent if its standard error ($SE$) approaches $0$ as the sample size ($n$) increases**:$$\lim_{n \rightarrow \infty}{SH(\hat{\theta})} = 0$$

That is, as the sample size grows, the value of the standard error gets closer and closer to the true population value of the measure ($\theta$).

It is easy to see that, as a function of the sample size ($n$), standard error of the mean is a hyperbolic function of the form $f(n) \sim \frac{1}{n}$, which approaches $0$ as $n$ tends to infinity:$$\lim_{n \rightarrow \infty}{SE(\bar{y})} = \lim_{n \rightarrow \infty}{\frac{\sigma}{\sqrt{n}}} = 0$$

### 2.3. Mean Squared Error (*MSE*)

So, we have concluded that the standard deviation of an estimator, so the **standard error of the estimator, only gives the expected deviation of a specific sample-based $\hat{\theta}$ from the true population $\theta$ if the estimator is unbiased**, because in this case, $\theta$ matches the mean of the $\hat{\theta}$ values calculated from many samples, i.e., $E(\hat{\theta})$.

If we are dealing with a biased estimator, then we can **manually calculate the expected squared deviation of the $\hat{\theta}$ against the true parameter $\theta$ from many samples. This measure is called the Mean Squared Error (MSE)**. To calculate it, we simply apply the classical variance formula of random variables as follows: $$MSE(\hat{\theta})=E\left(\left(\hat\theta-\theta\right)^2\right)$$


Now, **let's compute the $MSE$ for the only biased estimator we know of: the uncorrected sample standard deviation!** In R, we must use the `mean` function (as a "synonym" for the expected value in the formula above) and *manually* implement the formula, because using the built-in `sd` function or even our `original_sd` function (from Section 1) would compare to $E(\hat{\theta})$ instead of $\theta$, and due to bias, these two values do not coincide!

```{r}
MSE_uncorr_sd <- mean((exp_samples$orig_sd - sigma)^2)
MSE_uncorr_sd
```

Awesome! Now, **this $MSE$ value** is actually the **sum of the squared errors of two types of estimation errors** if we rearrange the formula of $MSE$ o bit. Specifically: $$MSE(\hat{\theta})=E\left(\left(\hat\theta-\theta\right)^2\right)=\left(E\left(\hat{\theta}\right)-\theta\right)^2+E\left(\left(\hat\theta-E\left(\hat{\theta}\right)\right)^2\right)=Bs^2+SE^2$$

Here, the **deviation of an estimator $\hat{\theta}$ from the true $\theta$ can be obtained in two steps**:

1. $SE$: How much does a specific sample’s $\hat{\theta}$ differ from the mean of the estimates, $E(\hat{\theta})$.
2. $Bs$: How much does the mean of the estimates differ from the true $\theta$: $Bs = E(\hat{\theta}) - \theta$

And if we calculate this in practice, we can see that the $MSE$ really is the **sum of these two types of errors**.

```{r}
Bs_uncorr_sd <- mean(exp_samples$orig_sd) - sigma
SE_uncorr_sd <- original_sd(exp_samples$orig_sd)

MSE_uncorr_sd_Sum <- Bs_uncorr_sd^2 + SE_uncorr_sd^2

c(MSE_uncorr_sd, MSE_uncorr_sd_Sum)
```

Wow, our summation logic actually works! :)

Of course, **when the estimation is unbiased, then $Bs=0$, so the identity $MSE = SE^2$ holds**. Let's take the **case of sample means**.

```{r}
MSE_mean <- mean((exp_samples$sample_mean - mu)^2)
SE_mean <- original_sd(exp_samples$sample_mean)

c(MSE_mean, SE_mean^2)
```

ExcellenT! We have the small numerical bias coming from the fact that we don't examine every possible sample, just 10000, but it doesn't really mess with our results, bless the Machine Spirit! :)

The $MSE$ is an excellent metric that can be used as a **measure to choose between multiple possible $\hat{\theta}$ estimator alternatives for a population parameter $\theta$.** A great **example of this is $(s^*)$ and $s$, as two alternative estimators for estimating the population standard deviation**, $\sigma$.

One might instinctively respond to this question by saying, **"but the unbiased estimator must be better"**. Well, not necessarily. Because **what if a biased estimator’s standard error is so much smaller than that of the unbiased estimator that it compensates for the bias**, and in the end, a single "shot" (i.e., an estimate $\hat{\theta}$) from the biased estimator is actually closer to the true $\theta$ than a "shot" from the unbiased estimator?

This can be well illustrated using another perspective on the wild boar hunting example from Section 2.2: **what if the unbiased estimator has such a large standard error that a specific "shot" (i.e., an estimate $\hat{\theta}$) is much farther from the true $\theta$ than an estimate from a biased estimator?**

<center>
![](BiasBoarCompare.png){width=90%}
</center>

<br>The figure shows that **estimates $\hat{\theta}_i$ from a slightly "left-biased" estimator with a small standard error still hit the wild boar, while the unbiased estimator, despite working very well on average over many samples, have such a large standard error for a single estimate $\hat{\theta}_i$ that they completely miss the wild boar!**

Thus, due to the above phenomenon, if **we need to choose between multiple estimators for a given $\theta$, we should base our decision on $MSE$**, as it simultaneously accounts for both the bias and the standard error.

The technical term for this is **efficiency**. If for two estimators, $\theta_1$ and $\theta_2$, it holds that $MSE(\theta_1)<MSE(\theta_2)$, then the estimator function $\theta_1$ is **more efficient** than $\theta_2$!!

Let's see this in practice: **What is the better estimator for the population standard deviation? The corrected or the uncorrected sample standard deviation?**

```{r}
# Biased Estimator = Uncorrected SD
MSE_uncorr_sd <- mean((exp_samples$orig_sd - sigma)^2)

# Unbiased Estimator = Corrected SD
MSE_corr_sd <- mean((exp_samples$sample_sd- sigma)^2)

# Result
c(MSE_uncorr_sd, MSE_corr_sd)
```

Oops, it seems that $MSE(s^*) < MSE(s)$, meaning that **in an "average" sample, the uncorrected sample variance is actually closer to the true population parameter $\sigma$ than the corrected version**. In other words, **the uncorrected sample standard deviation is more efficient than the corrected one!!**<br>
However, **systematic underestimation of a variable's dispersion and uncertainty is usually a bigger problem than a slightly higher standard error**. Therefore, **although overall, in terms of $MSE$, the sampling error of $s^*$ is smaller than that of $s$, the direction of the smaller error is "downward" due to bias**, and this is undesirable in this case. Instead, **we prefer to accept a somewhat larger but "symmetric" error**.<br>
As a result, **in the case of variance**, many practical applications "*override*" the $MSE$ decision, and the **corrected sample variance is used in most cases**. Or in other words, we can say that in the case of variance, we place more weight on $Bs^2$ than on $SE^2$ in the $MSE$, and we do not equally prefer their reduction.

On the other hand, at this point this is **not so suspicious** that the **maximum likelihood estimation for $\sigma$ in a normal distribution is the uncorrected standard deviation**! As it looks like the **more efficient estimator** for $\sigma$ in general.

**In the case if we need to choose between two unbiased estimators, we can simply choose the one with the smaller standard error, since in this case, it also means that its $MSE$ is smaller**, as $Bs^2=0$. **Only in this case** can we say that the efficiency criterion means that among two estimators, the one with the smaller $SE$ is the more efficient one.

## 3. Sampling Distribution of the Mean

Okay, let's take a look at the sampling distribution of the mean again, just like we did in Section 1.

```{r}
ggplot(exp_samples, aes(x=sample_mean)) + geom_histogram(aes(y = after_stat(density))) +
  geom_vline(xintercept = mu, color="red", linewidth=1)
```

We established back in Section 1 that this distribution looks just like **most beautiful normal distribution ever!**

Derived from CLT: $$\bar{y} \sim N\left(\mu,\frac{\sigma}{\sqrt{n}}\right)$$

Calculate theoretical SE of the mean.

```{r}
se_mean_theor <- sigma/sqrt(sample_size)
se_mean_theor
```

Show the sampling distribution, which is $N(100,10.54)$

```{r}
ggplot(data = exp_samples, mapping = aes(x=sample_mean)) + geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = se_mean_theor),
                col = 'red',
                linewidth = 1)
```

Get z-scores of the sample means and their distribution.

```{r}
exp_samples$z_score <- (exp_samples$sample_mean - mu)/se_mean_theor

ggplot(data = exp_samples, mapping = aes(x=z_score)) + geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1),
                col = 'red',
                linewidth = 1)
```


## 4. Confidence Interval of the Mean

Get t-scores of the sample means and their distribution.

```{r}
exp_samples$t_score <- (exp_samples$sample_mean - mu)/(exp_samples$sample_sd/sqrt(sample_size))

ggplot(data = exp_samples, mapping = aes(x=t_score)) + geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1),
                col = 'red',
                linewidth = 1) +
  stat_function(fun = dt, 
                args = list(df = sample_size-1),
                col = 'blue',
                linewidth = 1)
```

Confidence Interval formula for the mean: $$P\left(\bar{y}-t_{1-\frac{\alpha}{2}}^{n-1} \times \frac{s}{\sqrt{n}}< \mu <+t_{1-\frac{\alpha}{2}}^{n-1} \times \frac{s}{\sqrt{n}}\right) = 1- \alpha$$

See its hit rate for 98% confidence, so $\alpha=0.02$.

```{r}
t_mult <- qt(1-0.02/2, df = sample_size-1)
exp_samples$ci_mean_low_t <- exp_samples$sample_mean - exp_samples$sample_sd/sqrt(sample_size) * t_mult
exp_samples$ci_mean_upp_t <- exp_samples$sample_mean + exp_samples$sample_sd/sqrt(sample_size) * t_mult
mean((exp_samples$ci_mean_low_t <= mu) & (exp_samples$ci_mean_upp_t >= mu))
```


In large sample sizes it simplifies to: $$P\left(\bar{y}-z_{1-\frac{\alpha}{2}} \times \frac{s}{\sqrt{n}}< \mu <+z_{1-\frac{\alpha}{2}} \times \frac{s}{\sqrt{n}}\right) = 1- \alpha$$

See this hit rate also for 98% confidence, so $\alpha=0.02$.

```{r}
z_mult <- qnorm(1-0.02/2)
exp_samples$ci_mean_low_z <- exp_samples$sample_mean - exp_samples$sample_sd/sqrt(sample_size) * z_mult
exp_samples$ci_mean_upp_z <- exp_samples$sample_mean + exp_samples$sample_sd/sqrt(sample_size) * z_mult
mean((exp_samples$ci_mean_low_z <= mu) & (exp_samples$ci_mean_upp_z >= mu))
```

## 5. Sampling Distribution and Confidence Interval of MLEs

Sampling distribution definition: $$\hat{\theta}_{ML} \sim N(\theta,\sqrt{I^{-1}})$$


Get the empirical standard error for $\hat{\lambda}_{ML}$ from the simulations.

```{r}
exp_samples$est_rate <- 1/exp_samples$sample_mean
se_lambda_empir <- sqrt(mean((exp_samples$est_rate - rate)^2))
se_lambda_empir
```

See for the 666th sample that the $\sqrt{I^{-1}}$ approximation works.

Get the $\hat{\lambda}_{ML}$ numerically with Hessian matrix included.

```{r}
sample_666 <- as.numeric(t(exp_samples[666,1:sample_size]))

# function definition
neg_log_likelihood_expon <- function(theta_expon){
  log_lik_observations <- log(dexp(x = sample_666,
                                   rate = theta_expon))
  return(-sum(log_lik_observations))
}

# do the MLE with optim
expon_ml <- optim(0.001, neg_log_likelihood_expon, hessian = TRUE)
c(expon_ml$par, exp_samples$est_rate[666]) # check estimate
```

Get $SE\left(\hat{\lambda}_{ML}\right)$ from Hessian and see if its truly a good approximation of the standard error obtained from the simulations empirically.

```{r}
se_lambda_approx <- sqrt(solve(expon_ml$hessian))
c(se_lambda_approx[1,1], se_lambda_empir)
```

Difference is practically $0$ until 5 digits!

This way, we can get confidence intervals for $\lambda$. For example, on a 97% level.

```{r}
c(expon_ml$par - se_lambda_approx[1,1]*qnorm(1-0.03/2),
  expon_ml$par + se_lambda_approx[1,1]*qnorm(1-0.03/2))
```

Cramér-Rao inequality: $$SE\left(\hat{\theta}\right) \geq \sqrt{I^{-1}}$$

MLE is functional invariant. If $\hat{\theta}$ is MLE for $\theta$, then by definition $g\left(\hat{\theta}\right)$ is MLE for $g(\theta)$.

## 6. Covariance of Estimators

TODO

## 7. Case Study - Is Party Peference Influenced by Internet Usage Time?

Load the ESS data.

```{r}
ess <- readxl::read_excel("ess_hun_2024.xlsx")
str(ess)
```

Filter to valid internet usage time values.

```{r}
ess_small <- ess[!is.na(ess$DailyNetUse) & (ess$DailyNetUse > 0),
                 c("idno", "DailyNetUse", "PartyPreference")]
str(ess_small)
```

Fit MLE with `fitdistrplus`.

```{r}
library(fitdistrplus)
fit_lnorm <- fitdist(ess_small$DailyNetUse, "lnorm", method = "mle")
```

See the details.

```{r}
summary(fit_lnorm)
```

Do the MLE grouped by political party preference.

```{r}
lnorm_byparty <- data.frame(Party=unique(ess_small$PartyPreference),
                            meanlog = NA,
                            se = NA)

for (party in lnorm_byparty$Party) {
  fit_temp <- fitdist(ess_small$DailyNetUse[ess_small$PartyPreference==party],
                      "lnorm", method = "mle")
  lnorm_byparty$meanlog[lnorm_byparty$Party==party] = fit_temp$estimate["meanlog"]
  lnorm_byparty$se[lnorm_byparty$Party==party] = fit_temp$sd["meanlog"]
}

lnorm_byparty
```

Get CI with 95% level of confidence.

```{r}
lnorm_byparty$ci_low <- lnorm_byparty$meanlog - lnorm_byparty$se * qnorm(1-0.05/2)
lnorm_byparty$ci_upp <- lnorm_byparty$meanlog + lnorm_byparty$se * qnorm(1-0.05/2)
lnorm_byparty
```

Show CI result on barplot.

```{r}
ggplot(lnorm_byparty, aes(x = reorder(Party, meanlog), y=meanlog, fill = Party)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin=ci_low, ymax=ci_upp)) +
  coord_flip()
```

We can say that with 95% confidence that the Opposition spends more time a day on the internet than those who prefer Fidesz or whose preference is unknown.

