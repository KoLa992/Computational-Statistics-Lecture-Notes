---
title: "Basic Concepts of Hypothesis Testing"
author: "László Kovács"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify;
font-size: 12pt}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Fundamentals of Hypothesis Testing 

In essence, **hypothesis testing** is simply an **alternative method, alongside confidence intervals, for accounting for the sampling error of a statistical parameter**, when we aim to infer its true population value from its estimator calculated from a sample. However, the **concept of hypothesis testing approaches the problem somewhat differently compared to the confidence intervals we have studied in** <a href="Chapter04.html" target="_blank">Chapter 4</a>.

In hypothesis testing, we formulate a **statement (or, more formally, a hypothesis) about the true population value of a statistical parameter** (mean, standard deviation, $\lambda$, etc.), **and then we attempt to determine whether the observed sample data support or contradict this hypothesis**. At first glance, we focus specifically on parametric hypothesis tests, or in short, **parametric tests**, because the **hypothesis we formulate will always concern the true population value of a statistical parameter**.

Let's once again **examine an IID sample of size $n=120$ from the population with $Exp(0.01)$ distribution**. Let’s set the value in `set.seed` to $1992$ so that we all get the same random sample of $120$ elements!

```{r}
n <- 120
rate <- 0.01

set.seed(1992)
sample_from_exp <- rexp(n = 120, rate = rate)
hist(sample_from_exp)
```

Great, the histogram looks like a good old fashioned Exponential distribution with a long right tail.

Now then. Regarding the **true expected value of the unobserved population, I can formulate $6$ different statements** in relation to a specific value, e.g., $90$. As before, we denote the true population expected value of $1/\lambda=1/0.01=100$ by $\mu$:

1. I’m pessimistic and say that the true expected value in the unobserved population is less than $90$: $\mu < 90$
2. As an optimist, I could say that the true expected value is greater than $90$: $\mu > 90$
3. I can also imagine that the true expected value is exactly $90$ $\mu = 90$
4. Or I might believe that the unobserved population’s expected value is anything but $90$: $\mu \neq 90$
5. A semi-pessimist might say that the unobserved population’s expected value is at most $90$: $\mu \leq 90$
6. By elimination, the semi-optimist would think that the unobserved population’s expected value is at least $90$: $\mu \geq 90$

Alright, so I have 6 theories... or rather, *hypotheses*. Now, let’s check the **average in the observed $120$-element sample**.

```{r}
mean(sample_from_exp)
```

Based on the observed $120$ values, the sample mean is approximately $82.9$. At first glance, statement 1, the pessimistic one, seems true, as the **average is smaller than $90$**. BUT! **What we see here is only the average of $120$ obsrvations; the population’s true expected value could be different!**<br>
To put it more precisely, the **difference between $82.9$ and $90$ could simply be due to sampling error**!!<br>
**That’s why we perform hypothesis testing**: To determine whether the hypothesis $\mu < 90$ can be considered as valid, even considering sampling error.

In reality, if we think about it, all 6 statements boil down to the same fundamental question: Does the true population expecte value $\mu$ differ enough from $90$ that the **difference exceeds sampling error**? In statistical terms, we **test whether the difference between the true $\mu$ and $90$ is SIGNIFICANT**.

Here, we actually have the true population distribution that is $Y \sim Exp(0.01)$, so we actually know that in the unobserved population $\mu = 1/0.001 = 100$ minutes, which is **actually higher than** $90$!! So, in reality statement 2 is true!

However, **in practice, we DO NOT know this, because we only have the $n=120$ element sample!** So, our **only option is to use hypothesis testing to check whether the observed sample mean $\bar{y} = 82.9$ significantly differs from $90$**.

Following this logic, we define a **null hypothesis** $H_0$ and an **alternative hypothesis** $H_1$ **from our original statement**.

The key principle is the following. The null hypothesis $H_0$ always **allows equality for the examined statistical parameter**. The alternative hypothesis $H_1$ **depends on the original statement**:

- If the **original statement allows equality**, then it becomes $H_0$, and we **negate it in $H_1$**. In this case, we **expect $H_0$ to be true**, meaning that **the true population expected value does not significantly differ from $90$ minutes**.
- If the **original statement does not allow equality**, then it moves to $H_1$, and we **negate it in $H_0$**. Here, we **expect $H_1$ to be true**, meaning that **the true population expected value significantly differs from $90$ minutes**.

Using these principles, we can construct the following $H_0$ and $H_1$ pairs for our $6$ original statements:

1. Statement: $\mu < 90$ || $H_0:\mu \geq 90$ || $H_1:\mu < 90$ || Statement is in $H_1$
2. Statement: $\mu > 90$ || $H_0:\mu \leq 90$ || $H_1:\mu > 90$ || Statement is in $H_1$
3. Statement: $\mu = 90$ || $H_0:\mu = 90$ || $H_1:\mu \neq 90$ || Statement is in $H_0$
4. Statement: $\mu \neq 90$ || $H_0:\mu = 90$ || $H_1:\mu \neq 90$ || Statement is in $H_1$
5. Statement: $\mu \leq 90$ || $H_0:\mu \leq 90$ || $H_1:\mu > 90$ || Statement is in $H_0$
6. Statement: $\mu \geq 90$ || $H_0:\mu \geq 90$ || $H_1:\mu < 90$ || Statement is in $H_0$

In statements 1, 2, 5, and 6, where **$H_0$ does not have a strict equality sign, we often define a technical null hypothesis $H_0^T$**. This simply means that we **rewrite $H_0$ using a strict equality sign**.<br>
For example, **for statements 1 and 5, the technical null hypothesis is**:

1. Statement: $\mu < 90$ || $H_0^T:\mu = 90$ || $H_1:\mu < 90$ || Statement is in $H_1$
5. Statement: $\mu \leq 90$ || $H_0^T:\mu = 90$ || $H_1:\mu > 90$ || Statement is in $H_0$

Statistical **tests** (also called hypothesis tests) **are categorized based on the relational sign in $H_1$**:

- If $H_1$ contains $\neq$, then it is a **two-tailed test**
- If $H_1$ contains $<$, then it is a **left-tailed test**
- If $H_1$ contains $>$, then it is a **right-tailed test**

### 1.1. The concept of p-value

If we have properly written down the corresponding $H_0$ and $H_1$ pairs, we then determine which one is considered true based on our observed sample. For this purpose, we use a **statistical measure called the p-value**.  

The **p-value tells us the probability that, assuming $H_0$ is TRUE, we obtain a deviation from $H_0$ that is GREATER than what we observed in our sample data**. Clearly, the value of the statistical measure calculated from the sample (estimator) will never be exactly equal to what we stated in $H_0$. The question is, **what is the probability that a random new sample will produce a larger deviation from $H_0$ than what we observed in the current sample?** This probability is expressed by the so-called **p-value**: $$P(OtherRandomDifference>MyDifference|H_0)$$

So, if we obtain, for example, **p-value = 30%**, then we can say that **there is a 30% probability that another sample will show a greater deviation from $H_0$ than what we observed in our own data**. Based on this, we tend to accept $H_0$, because if we assume it to be true, then we would expect to see even greater deviations from $H_0$ in other random samples with a fairly high probability. Thus, **the deviation we observed is not large enough compared to $H_0$ (not significant), so we do not reject it, because our data does not provide enough evidence for such a decision**.  

However, if we obtain, for example, **p-value = 0.1%**, then we can say that **there is only a 0.1% probability that another sample will show a greater deviation from $H_0$ than what we observed in our own data**. Based on this, we tend to reject $H_0$, because in this case, assuming $H_0$ to be true, we would very rarely see a greater deviation from $H_0$ than what we actually observed. Therefore, **the deviation we observed compared to $H_0$ is large enough (significant), so we confidently reject it, as our data provides sufficient evidence for this decision**.  

Based on all this, the **decision rule** between $H_0$ and $H_1$ is as follows.

If the **p-value is too high, we accept** $H_0$, because we do not have enough evidence from the observed sample for rejecting it. Conversely, if the **p-value is too low, we reject** $H_0$ and **accept** $H_1$. 

The question is: how do we decide whether a p-value is too high or too low? For this, we use the so-called **significance level, denoted by** $\alpha$. The significance level represents the **maximum accepted probability of error when rejecting $H_0$**.  

So, if we say that $\alpha = 5\%$, then for a p-value below $5\%$, we **reject $H_0$, because the observed data shows a large enough deviation from $H_0$ that we can ensure a maximum error probability of $5\%$.** On the other hand, if our p-value is above $5\%$, we **accept $H_0$, because the observed data does NOT show a large enough deviation from $H_0$ to justify rejection with at most a $5\%$ error probability.**  

At this point, we might **believe the following**:  

- **p-value** $> \alpha \rightarrow H_0$  
- **p-value** $\leq \alpha \rightarrow H_1$ 

But of course, **LIFE IS NEVER THAT SIMPLE!!** The main problem here is that **our decision could heavily depend on what exact value we set for $\alpha$!** For example, say we set our maximum allowed error probability to $\alpha = 5\%$. If we then obtain a p-value of $4.8\%$, we would reject $H_0$ under $\alpha = 5\%$, but under $\alpha = 3\%$, we would accept $H_0$.<br>
Because of this, we typically apply a **range of common significance levels, which fall between $1\%$ and $10\%$**. We do not want to allow an error probability above $10\%$, as that would be unreasonably high. On the other hand, an error probability of **$0\%$ is only possible if we observe the entire population and know the true population value of the examined statistical parameter**. At that point, however, we lose the whole point of sampling — namely, that we do not have to observe every past and future data point to determine the true population value of our statistic. This $ \alpha = 0\%$ case is similar to why a $100\%$ confidence interval is meaningless — because it would say that based on our sample, the parameter could be anywhere between $\pm \infty$... :)<br>
Furthermore, we will later see from **simulations that both true and false $H_0$ statements can easily produce p-values between $1\%$ and $10\%$ in reality!**  

So, based on this long-winded explanation, a more reasonable solution seems to be to **not make a decision between $H_0$ and $H_1$ when the p-value is between $1\%$ and $10\%$, because the decision would be too sensitive to the exact choice of $\alpha$**.<br>
If our sample data produces a p-value in the range $1\%$ to $10\%$, the best thing to do is **increase the sample size ($n$) until the p-value clearly falls below $1\%$ or rises above $10\%$**. This method is called <a href="https://en.wikipedia.org/wiki/Sequential_analysis" target="_blank">sequential analysis</a>, and it is widely used in clinical drug trials and industrial quality control. The method was originally developed by a Hungarian statistician, <a href="https://en.wikipedia.org/wiki/Abraham_Wald" target="_blank">Abraham Wald</a>, for the U.S. Air Force during World War II (since he was Jewish, he was not "needed" in Hungary at the time). So, like everything important in the world, this too was invented by Hungarians. :)  

Thus, the **more correct decision rules in hypothesis testing** are:  

- **p-value** $>10\% \rightarrow H_0$
- $1\% <$ **p-value** $\leq 10\% \rightarrow$ **no decision**
- **p-value** $\leq1\% \rightarrow H_1$ 

Another headache is that we must be very cautious with p-values between $1\%$ and $10\%$, because **p-values and $\alpha$ only describe one type of decision error in hypothesis testing!**  The **p-value and $\alpha$ only provide information about the so-called TYPE I ERROR: the probability of rejecting a true $H_0$.**<br>
BUT we also have a **TYPE II ERROR**, which is **the probability of mistakenly accepting a false $H_0$.** The bad news is that **we cannot estimate or control the probability of Type II error based on our observed sample!**<br>
This is the major downside of hypothesis testing: if **we accept $H_0$, we do not actually know how likely we are to be wrong — we only know the maximum probability of rejecting a true $H_0$, because that is the significance level ($\alpha$).**<br>
For this reason, many statisticians phrase it carefully, saying that we do not "accept" $H_0$, but rather, **we "fail to reject" $H_0$.**  

For a better understanding of the problem, I think the following meme will be of great help. :)

<center>
![](Type2.png){width=40%}
</center>

<br>Now, let’s see **how to calculate the p-value based on an observed sample when the statistical parameter of interest is the expected value**, i.e., $\mu$, and the **estimator is the sample mean**, i.e. $\bar{y}$!

## 2. The *t-test* for the Expected Value

The calculation of the p-value **always requires a statistical measure called a test statistic, which we can always compute solely from the observed sample data**. Then, **the p-value is calculated from the test statistic based on a probability distribution (standard normal, t-distribution, chi-square, etc.)**.

Let us denote the **HYPOTHETICAL value of the expected value** as $\mu_0$. This is the value that I **assume** in the statement about the true population expected value. In Section 1, this was $90$ minutes. With this notation, the test statistic for the mean appears as follows: $$\frac{\bar{y}-\mu_0}{\frac{s}{\sqrt{n}}}$$

Thus, our test statistic is **simply the difference between the sample mean and the hypothetical expected value, divided by the standard error of the mean**.

If our reasoning in $H_0$ is correct, and **we truly managed to take the real population expected value $\mu$ as the theoretical** (hypothetical) **expected value $\mu_0$, then with lots of repeated samples, our test statistic follows a t-distribution with $n-1$ degrees of freedom**: $$\frac{\bar{y}-\mu}{\frac{s}{\sqrt{n}}} \sim t(n-1)$$

The principle behind this formula is that **we apply the technical null hypothesis $\mu = \mu_0$ ($H_0^T$) in all six different types of $H_0$ and $H_1$ pairs, so we can determine the distribution of the test statistic across many samples**.

We can quickly verify this by **drawing $10000$ samples of size $n=120$ from the $Y \sim Exp(0.01)$ distribution**, as we did in <a href="Chapter04.html" target="_blank">Chapter 4</a>.

```{r}
rate <- 0.01
mu <- 1/rate
sample_size <- 120
replication_number <- 10000

set.seed(1992)
exp_samples <- replicate(replication_number, rexp(n = sample_size, rate = rate))
exp_samples <- as.data.frame(t(exp_samples))

rownames(exp_samples) <- paste0("Sample",1:replication_number)
colnames(exp_samples) <- paste0("Element",1:sample_size)
head(exp_samples)
```

Okay, from the results, we see that the data frame is structured such that **each row contains one sample of 120 observations, and the observations are stored in columns**.

We compute the sample mean and corrected standard deviation for each sample. Be sure that the second parameter of the `sapply` function is `1`, so that we take means and standard deviations row-wise instead of column-wise. Also, ensure that the functions are applied only to the first $120$ columns, as those contain the actual sample observations. Basically, do everything exactly as we did in <a href="Chapter04.html" target="_blank">Section 1 of Chapter 4</a>.

```{r}
exp_samples$sample_mean <- apply(exp_samples[,1:sample_size], 1, mean)
exp_samples$sample_sd <- apply(exp_samples[,1:sample_size], 1, sd) # corrected -> unbiased

head(exp_samples[,(sample_size-1):ncol(exp_samples)]) # look at the last 4 columns
```

Then, we can **compute our test statistic for two pairs of $H_0$ and $H_1$, where we know that $H_0$ is true in one case and false in another**:

- Case of **TRUE** $H_0$: $H_0:\mu=100$ and $H_1: \mu < 100$
- Case of **FALSE** $H_0$: $H_0^T:\mu=70$ and $H_1: \mu > 70$

```{r}
mu_0_true <- mu # true population mean
mu_0_false <- 70 # arbitrary value

exp_samples$test_stat_H0true <- (exp_samples$sample_mean - mu_0_true)/(exp_samples$sample_sd/sqrt(sample_size))
exp_samples$test_stat_H0false <- (exp_samples$sample_mean - mu_0_false)/(exp_samples$sample_sd/sqrt(sample_size))

head(exp_samples[,(sample_size-1):ncol(exp_samples)])
```

Alright! Now that we have the test statistics for both scenarios, **let's examine how the histograms of these test statistics from $10000$ samples compare to the density function of the $t(n-1)$, i.e., $t(100-1)$, distribution**.<br>  
The code for visualizing the fit of the density function to the histogram follows the exact logic of the code used to visualize sampling distributions in <a href="Chapter04.html" target="_blank">Section 1 of Chapter 4</a>. Basically, what we are going to see here are the **sampling distributions for the two test statistics**.

```{r warning=FALSE}
library(ggplot2)

# general ggplot function without defining any axes
ggplot(exp_samples) +
  # histogram of test statistic when H0 is TRUE (x axis)
  geom_histogram(aes(x = test_stat_H0true, y = after_stat(density), fill="H0 TRUE"), alpha=0.6) +
  # histogram of test statistic when H0 is FALSE (x axis)
  geom_histogram(aes(x = test_stat_H0false, y = after_stat(density), fill="H0 FALSE"), alpha=0.3) +
  # adding the density function of the t(n-1) distribution
  stat_function(fun = dt, 
                args = list(df = (n-1)),
                col = 'blue', linewidth = 1)
```

Great! So **when $H_0$ is true, the test statistic does indeed follow a t-distribution in our repeated sampling experiment**. When $H_0$ is false, the histogram forms some other symmetric distribution that is most definitely NOT a t-distribution as the mode is not at $0$. However, we are not particularly concerned with what this distribution is. :)

From the fact that the test statistic follows a t-distribution under repeated sampling when $H_0$ is true, **we can compute the p-value**. Essentially, we calculate areas under the *t-distribution* density function. To do this, we must consider **what the best-case scenario for $H_0$ is regarding the test statistic**:

- **Two-tailed** tests ($H_0: \mu=\mu_0$ and $H_1: \mu \neq \mu_0$): If the **test statistic is exactly** $0 \rightarrow$ this theoretical case suggests that the hypothetical expected value and the true population expected value are identical, so with probability $1$, I would obtain greater deviations from $H_0$ in other samples.
- **Left-tailed** tests ($H_0: \mu\geq\mu_0$ and $H_1: \mu < \mu_0$): If the **test statistic =** $+\infty \rightarrow$ this theoretical case suggests that the population expected value ($\mu$) is exactly $+\infty$, making every $\mu_0$ smaller, so with probability $1$, I would obtain greater deviations from $H_0$ than what's observed.
- **Right-tailed** tests ($H_0: \mu=\mu_0$ and $H_1: \mu > \mu_0$): If the **test statistic =** $-\infty \rightarrow$ this theoretical case suggests that the population expected value ($\mu$) is exactly $-\infty$, making every $\mu_0$ larger, so with probability $1$, I would obtain greater deviations from $H_0$ than what's observed.

Based on this, if **we have a known test statistic value denoted as $t$, then the p-value is calculated from the $t(n-1)$ distribution as shown in the following figure**:

<center>
![](pval_fromt.jpg){width=90%}
</center>

<br>As seen, **in all three cases, the p-value is computed such that the further the specific test statistic $t$ moves away from the best case for $H_0$, the smaller the p-value becomes $\rightarrow$ it becomes less and less likely to obtain greater deviations from $H_0$ than observed!!**

Based on all this, let's **calculate the p-value** using the `pt` function **for all $10000$ samples for the two previously examined pairs of $H_0$ and $H_1$, where $H_0$ was true in one case and false in the other**:

- **TRUE** $H_0$ case: $H_0:\mu=100$ and $H_1: \mu < 100$
- **FALSE** $H_0$ case: $H_0:\mu=70$ and $H_1: \mu > 70$

Note that the `1-` part before the `pt` function is included for the false $H_0$ case because it is a **right-tailed test**.

```{r}
exp_samples$p_value_H0true <- pt(exp_samples$test_stat_H0true, df = (sample_size-1))
exp_samples$p_value_H0false <- 1-pt(exp_samples$test_stat_H0false, df = (sample_size-1))

head(exp_samples[,sample_size:ncol(exp_samples)])
```

Great! Based on the first six samples, the expected p-value results were obtained: high values when $H_0$ is true, low values when $H_0$ is false. To interpret more precisely, let's examine the **4th sample**:

- **When $H_0$ is true, there is a 30.79% probability of obtaining greater deviations from $H_0$ than observed in sample 4**.
- **When $H_0$ is false, there is only a 0.07% probability of obtaining greater deviations from $H_0$ than observed in sample 4**.

At the same time, in sample $1.$, we see that the p-value is only $1.9\%$ when $H_0$ is true. So, the rule **"we do not make a decision if the p-value is between $1\%$ and $10\%$" seems quite reasonable**! :)

Similarly, the caution taken for p-values in the range of $1\%$ to $10\%$ is reinforced when we examine the p-values for true and false $H_0$ using a box plot.

```{r}
boxplot(exp_samples[,c("p_value_H0true", "p_value_H0false")])
```

As expected, when $H_0$ is true, the middle $50\%$ of p-values, that is, the interquartile range ($IQR$), is substantially higher than in the case of false $H_0$. However, it is also very important to observe in the plot that **large p-values are considered outliers when $H_0$ is false, but small p-values are not outliers when $H_0$ is true!** Thus, sometimes it may be worth using a stricter $\alpha$ than $1\%$. Many people reject $H_0$ only if the p-value is smaller than $0.1\%=0.001$, meaning they work with $\alpha=0.001$.

Based on our results, we can calculate the proportion of decision errors when $\alpha=5\%$ and $H_0$ is true. That is, the **empirical probability of a Type I error**: how often did we reject a true $H_0$? According to the definition of the significance level, this proportion should be around $5\%$.

```{r}
mean(exp_samples$p_value_H0true<0.05)
```

Well, the result is $7.55\%$, which is 2.55 percentage point higher than $5\%$, but not by much. It looks like the system is working properly. If we examined even more samples than 10000, this proportion would get closer and closer to $5\%$.

Moreover, since in this simulation case, we know when $H_0$ is true or false, we can now also calculate the **empirical probability of a Type II error** at $\alpha=5\%$: the proportion of times we accept a false $H_0$.

```{r}
mean(exp_samples$p_value_H0false>=0.05)
```

Not bad, $2.22\%$!

Let us observe the completely logical relationship that if the **probability of a Type I error (significance level) decreases, then the probability of a Type II error increases**. Now, let's calculate the two empirical error probabilities at $\alpha=1\%$. That is, we lowered the expected probability of a Type I error to $1\%$.

```{r}
mean(exp_samples$p_value_H0true<0.01) # empirical probability of Type I error
mean(exp_samples$p_value_H0false>=0.01) # empirical probability of Type II error
```

### 2.1. The *t-test* with a bulit-in R function

Luckily, we have an advantage because the **p-value of the t-test for the mean can be nicely computed using a built-in R function**!

For example, let’s take the $n=120$ sample taken at the beginning of Section 1, which we stored in the R object `sample_from_exp`, which is a vector of size 120.

```{r}
str(sample_from_exp)
```

Examine the following $H_0$ + $H_1$ pair for this specific sample:

- $H_0^T:\mu=60$
- $H_1 > 60$

Since the $H_1$ hypothesis contains the $>$ relational sign, the hypothesis test will be **right-tailed**.<br>
We know that here **$H_1$ is true, as we have previously seen that the true population expected value is $100$**. Therefore, we expect **a very low p-value**!

This result can be nicely obtained manually! First, we calculate the **test statistic** $\frac{\bar{y}-\mu_0}{\frac{s}{\sqrt{n}}}$ assuming $\mu_0=60$, since in hypothesis testing we examine whether the true population expected value is significantly greater than this assumed (hypothetical) $60$.

```{r}
hypothetical_exp_value <- 60
sample_mean <- mean(sample_from_exp)
s <- sd(sample_from_exp)
n <- length(sample_from_exp)

test_stat <- (sample_mean - hypothetical_exp_value) / (s/sqrt(n))
test_stat
```

Great! From this, we can quickly obtain the p-value from the $t(n-1)$ distribution using the `pt` function. Here, we are calculating **the probability of falling above the test statistic, since this is a right-tailed test**.

```{r}
p_val_t_distr = 1 - pt(test_stat, df = n-1)
p_val_t_distr * 100 # in percentage format
```

**Based on the p-value, we can state that for the examined sample of size $100$, there is only a $0.302\%$ probability that another random sample would show a greater deviation from $H_0$ than what we observed here**. This **is even smaller than the smallest common significance level of $\alpha=1\%$, so we make the decision to reject $H_0$**.<br>
Based on this, $H_1$ is acceptable, meaning that the **true expected value is significantly** (beyond sampling error) **greater than $60$**.<br>
Now we know that **we made the correct decision**, as we have seen that the population expected value is $100$.

Fortunately, all of this can also be handled using the `t.test` function. If the function is provided with:

- the column containing the observations of our current sample from a data frame as its first parameter,
- the theoretical expected value, i.e., $\mu_0$, in the `mu` parameter,
- the test direction in the `alternative` parameter based on the relational sign in $H_1$
  * possible values: `'two.sided'`, `'less'`, `'greater'`, corresponding to the relational signs $\neq$, $<$, and $>$ in $H_1$

Then the **function provides the p-value of the t-test and the test statistic as well**.

```{r}
t.test(sample_from_exp, mu = 60, alternative = "greater")
```

Awesome, we get the same test statistic ($2.7954$) and the p-value of $0.302\%$! :)

### 2.2. Assumptions of the *t-test*

Of course, this t-test-based hypothesis test also has prerequisites, just as different confidence interval formulas do. **If these conditions are not met, then no matter what the p-value is, it cannot be used to make a decision about $H_0$ and $H_1$**, because the underlying assumptions of the applied formulas are NOT met by the data.

The t-test essentially has **two conditions** to be considered:

1. If we have a **small sample** ($n<100$), we assume that **the data from which the sample was drawn follows a normal distribution**.
2. If the **sample size is large** ($n\geq100$), then **no additional assumptions need to be considered**.

Now, our sample of $n=120$ from and $Exp(0.01)$ population is already a large sample, since $120\geq100$, so the long right tail of the exponential distribution does not bother us much. :)

## 3. The *z-test* for the Population Mean

Of course, in the case of confidence intervals for the mean, we observed that **for a large degree of freedom ($df \geq 100$), the t-distribution is essentially the same as the standard normal distribution**.

That is, to put it a bit more mathematically: **if $n \geq 100$, then under the true null hypothesis ($H_0$)** (i.e., when we substitute the true population expected value: $\mu$, for $\mu_0$), our **test statistic follows a standard normal ($N(0,1)$) distribution**: $$\frac{\bar{y}-\mu}{\frac{s}{\sqrt{n}}} \sim N(0,1)$$

Based on this, we can **compute the p-value from the test statistic using exactly the same principle we applied in Section 2 for the t-distribution**. Since the null and alternative hypotheses remain unchanged ($H_0^T: \mu = 60$ and $H_1 > 60$), we still have a right-tailed test, and since the sample elements remain the same in our 100-element sample, the **test statistic is also the same as the one used in Section 2.1**.<br>
**Since this remains a right-tailed test**, we must **determine the probability of falling above the test statistic in the standard normal distribution** using the `pnorm` function with the $1-$ version.

```{r}
p_val_norm_distr <- 1 - pnorm(test_stat)
c(p_val_norm_distr * 100,
  p_val_t_distr * 100)
```

Both p-values are around $0.3\%$, so there really is no noticeable difference between the standard normal and t-distribution when calculating the p-value.

## 4. Relationship of Type I & II Errors with Sample Size

TODO

```{r}
p_values <- sapply(seq(10,120, by = 5), function(x) t.test(sample_from_exp[1:x], mu = 60, alternative = "greater")$p.value)
df_pvals <- data.frame(sample_size = seq(10,120, by = 5),
                       p_val = p_values)

ggplot(df_pvals, aes(x=sample_size, y=p_val)) + geom_line(linewidth=1)
```


## 5. Other Important Parametric Tests

TODO

### 5.1. Two-Sample Welch *t-test*

TODO

### 5.2. Analysis of Variances (ANOVA)

TODO

## 6. Non-Parametric Tests

TODO

### 6.1. Goodness-of-Fit Tests

TODO

### 6.2. Tests of Homogeneity

TODO

## 7. Generalised Likelihood Ratio Tests (GLRTs)

TODO