---
title: "Maximum Likelihood Estimators"
author: "László Kovács"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

<style>
body {
text-align: justify;
font-size: 12pt}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. General Principles of Maximum Likelihood (ML) Estimation

So, let's imagine that we're in the same situation like in <a href="Chapter01.html" target="_blank">Chapter 1</a>: we would like to predict how a statistical variable (age, income, daily internet usage time, BMI, etc.) behaves for unobserved population data. We apply theoretical probability distributions for this purpose.<br>
First, we check the observed histogram of the variable in question, and try to get a sense of how the empirical distribution of the variable behaves. Then, if from this histogram we we somehow know or suspect that our variable has Negative Binomial Distribution, then the next question is that what are the $r$ and $p$ parameter that best fit for our observed data. Or if our data is assumed to have Exponential distribution, then what is the $\lambda$ that best fits our observed sample data.

An important notation in this chapter that the **true parameters of our variable's distribution are denoted as $\theta$. This is a collective term, and its specific meaning depends on the theoretical distribution in question.** Like in the previous examples, if we suspect our data to have a Negative Binomial distribution, then $\theta=\{r,p\}$, but if an Exponential distribution is assumed, then we simply have $\theta=\lambda$. Based on <a href="Chapter01.html" target="_blank">Chapter 1</a>, we would have $\theta=\{\mu,\sigma\}$ for normal and lognormal distributions and $\theta=\lambda$ again for the Poisson distribution.

We can **never actually know the true $\theta$ if we have unobserved data for the variable**, but we can give reasonable **estimates or estimators for its value that are denoted as $\hat{\theta}$.** The $\hat{}$ symbol denotes that the values we have as the parameter values of the distribution are NOT its true values, but our "*best guess*" (more scientifically: estimators/estimates) for their values based on the observed data that we have.

The **method of moments** (MM) introduced in <a href="Chapter01.html" target="_blank">Chapter 1</a> is **one way to give estimators** of true distribution parameters as we've seen. This method has **one** great **advantage**: its **calculations are very-very simple**! However, it **does NOT consider every observation in our sample when estimating the distribution parameters**. It only uses calculated statistical measures of our sample, its moments. This can heavily influence the results in some cases. We've seen for the lognormal distribution (<a href="Chapter01.html" target="_blank">Section 4.2.of Chapter 1</a>) that using the classical MM method with the 1st and 2nd moments given horrible $\hat{\theta}$ estimators and we needed to use the 1st moment and the median to give better estimates.

The **maximum likelihood method uses every observation in our sample directly when estimating the distribution parameters** and avoids the problems caused by only concentrating on special properties of the distribution like moments. Maximum likelihood estimates (MLEs) have other advantages as well compared to other methods that are going to be discussed in <a href="Chapter04.html" target="_blank">Chapter 4</a>. On the other hand, its **calculations are more complex** than those of the method of moments.

Now, after seeing its benefits, let's see the **general principles of the maximum likelihood method**.

The basic concept is quite simple: let's **find those parameters** for a given probability distribution that **maximize the probability of all our observed data occurring in an IID sample** (i.e. random sample with replacement) from that given distribution. So, if the elements of the observed sample of size $n$ are denoted as $y=(y_1,y_2,...,y_n)$, and the parameters of the probability distribution we are looking for denoted as $\theta$, then we are looking for the $\theta$ values that maximizes the probability of $y$ occurring in case of $n$ random draws from the examined distribution.<br>
A mathematical formulation for this task is as follows: $$P(y|\theta) \rightarrow \max_{\theta}$$

Here, the $P(y|\theta)$ probability is called as the **likelihood of the sample**. This is the **probability** of our observed **sample occurring on the condition that we give some arbitrary values for $\theta$.** So, in other words, we are **looking for those $\theta$ conditions that maximize our $y$ sample occurring.**<br>
To calculate this, we can **use the $f(x,\theta)$ mass/density functions of the discrete/continuous distributions**, as they **show the probability of an arbitrary number $x$ occurring in a random draw from the probability distribution with $\theta$ parameters**. So if we say that every $y_1,y_2,...,y_n$ observations of our sample are from the same probability distribution with the same mass/density function denoted as $f$, then this function can be used to give the **probability of the a specific $y_i$ sample observations occurring in a random draw as $f(y_i, \theta)$.**

So, the **probability of each $y_i$ observation of our sample occurring for a random draw under some $\theta$ parameter conditions** is given as $$f(y_i)=P(y_i|\theta)$$

This is the **likelihood of the $y_i$ observation**.

The **likelihood of the whole sample is the probability of all the observed $y_i$ values ($y_1,y_2,...,y_n$) occurring all at once!** This is the **PRODUCT of the $P(y_i|\theta)$ probabilities if the $y_i$ values are independent**, and since **we assume that we work with IID** (independent, identically distributed) **samples, the independence of the $y_i$ values can also be assumed**, because of the first "I" in IID. :)

Therefore, the **likelihood of our whole $y$ sample under the condition of some $\theta$ parameters in an IID sample of size $n$** is given as follows: $$L(\theta)=\prod_{i=1}^{n}{P(y_i|\theta)}=\prod_{i=1}^{n}{f(y_i, \theta)}$$

And we need to **maximize this $L(\theta)$ by varying the $\theta$ conditions**. As this is what is the **direct expression of the maximum likelihood principle**: *find those parameters that maximize the probability of all our observed data occurring in an IID sample*.

To **solve this maximization task**, we need to take the $\frac{\partial L(\theta)}{\partial\theta}$ derivatives and make them equal to $0$. However, **taking the derivative of a product-function that is $L(\theta)$ is rather painful**. Best to avoid this. :) And thankfully we can avoid this by **maximizing the log-likelihood function of the sample**, as **on a log-scale a multiplicative function becomes an additive one!** $$l(\theta)=\ln(L(\theta))=\ln\prod_{i=1}^{n}{f(y_i,\theta)}=\sum_{i=1}^{n}{\ln(f(y_i,\theta)})$$

So, in practice, to **make our jobs easy, we maximize the log-likelihood function in $\theta$:** $$l(\theta) \rightarrow \max_{\theta}$$

And we can simply do this maximization by **taking the derivative of the log-likelihood** according to $\theta$ and solve the equation of **this derivative being equal to $0$:** $$\frac{\partial l(\theta)}{\partial\theta}=0$$

Ok, great: we now know what to do to get maximum likelihood (ML) estimators. Let's see some **practical examples** for this maximum likelihood estimation process!

## 2. Exponential Distribution

Let's use our examples again from the <a href="https://github.com/KoLa992/Computational-Statistics-Lecture-Notes/blob/main/ess_hun_2024.xlsx" target="_blank">ess_hun_2024.xlsx</a> file that contains a dataset with the responses of 2048 Hungarian participants to 29 questions (plus an id column) from the 2024 European Social Survey (ESS2024). This is exactly the same dataset that we've used in <a href="Chapter01.html" target="_blank">Chapter 1</a>. So, if an empty value is found in any column, it means that the respondent in that row did not answer the question. The respondents in the database can be considered a **random sample** drawn **from** the entire **Hungarian population** aged 18 and over. Detailed description of the 29+1 variables in the dataset are given in <a href="Chapter01.html" target="_blank">Section 1 of Chapter 1</a>.

```{r}
ess <- readxl::read_excel("ess_hun_2024.xlsx")
str(ess)
```

If we remember, we've used the alcohol consumption on weekends (in grams) as our example for Exponential distribution in <a href="Chapter01.html" target="_blank">Section 4.1 of Chapter 1</a>. We've removed the missing values from our table back then, so let's do the same thing now.

```{r}
ess_small <- ess[!is.na(ess$AlcoholWeekend),c("idno", "AlcoholWeekend")]
hist(ess_small$AlcoholWeekend)
```

We can see the long right tail of the exponential distribution on the histogram: the majority of alcohol consumption (roughly 600) are within 50 grams, but the remaining observations all exceed this value, and there are even some extreme alcoholics with weekend consumption above 200 grams, even 1 or 2 in the 600-700 grams range.

The parameter to estimate for the Exponential distribution is $\lambda$. So, in the exponential case: $\theta=\lambda$.

Now, let's write the negative log-likelihood function for our sample as a custom R function. We need the **negative log-likelihood** as the **built-in methods for optimization in R can only minimize** a function, and NOT maximize. So, we'll **minimize the negative log-likelihood which is the same as maximizing the original log-likelihood**.<br>
The input parameter of the function, `theta_expon` is treated as the value for the $\lambda$ parameter of course. We can use the `dexp` function for calculating the $P(y_i|\theta)=f(y_i,\theta)$ probabilities for the Exponential distribution. Then, we just apply the $l(\theta)=\sum_{i=1}^{n}{\ln(f(y_i,\theta)})$ formula.

```{r}
# function definition
neg_log_likelihood_expon <- function(theta_expon){
  log_lik_observations <- log(dexp(x = ess_small$AlcoholWeekend,
                                   rate = theta_expon)) # log-probability of every observations occurring under the current parameters
  return(-sum(log_lik_observations))
}

# function test on some random parameter value
neg_log_likelihood_expon(0.001)
```

Do the **optimization with R's built-in `optim` function**. The first parameter of this function is the starting $\lambda$ value for the optimization. This value can be any arbitrary starting value, but we should respect the constraint on the Exponential parameter: $\lambda>0$. We can *ignore the possible warnings*, it just means that R encountered some invalid parameter values when trying out different $\lambda$ values during the optimization. R automatically ignores these cases, they won't cause any problems.

```{r warning=FALSE}
expon_ml <- optim(0.001, neg_log_likelihood_expon)
expon_ml$par # optimal parameter value
expon_ml$val # minimized negative log-likelihood of the sample
```

So, our result is $\hat{\lambda}_{ML}=0.0214$. Which is the **reciprocal of the sample mean and not the standard deviation**!

```{r}
1/mean(ess_small$AlcoholWeekend)
1/sd(ess_small$AlcoholWeekend)
```

As the ML method takes every observation into account when estimation a distribution parameter and not just utilizes a few moments, we can finally confirm that we **need to take the reciprocal of the mean and NOT the standard deviation when estimating $\lambda$ of an exponential distribution** despite thae fact that theoretically $E(Y)=D(Y)=\lambda$ for a $Y \sim Exp(\lambda)$ distribution.

### 2.1. Analitical Solution for the Exponential Distribution

The previous result can also be derived analytically by **taking the derivative of the log-likelihood** according to $\theta$ and solve the equation of **this derivative being equal to $0$:** $$\frac{\partial l(\theta)}{\partial\theta}=0$$

The $P(y_i|\theta)=f(y_i,\theta)$ probabilities for the Exponential distribution (where $\theta = \lambda$) are given by the Exponential density function: $$f(y_i, \lambda)=\lambda e^{-\lambda y_i}$$

The likelihood of the whole sample is the individual $y_i$ likelihoods multiplied together because of the $y_i$s independence (IID sample): $$L(\lambda)=\prod_{i=1}^n{\lambda e^{-\lambda y_i}}$$

We can expand the product by using the fact that multiplication becomes additive in the power: $$L(\lambda)=\lambda^n e^{-\lambda \sum_{i=1}^n{y_i}}$$

Taking the natural logarithm of the likelihood function to produce the log-likelihood: $$l(\lambda)=\ln(L(\lambda))=\ln(\lambda^n e^{-\lambda \sum_{i=1}^n{y_i}})$$

Using logarithm properties we can make the expression additive, so its easier to derivative: $$l(\lambda)=n \times \ln(\lambda) - \lambda \times \sum_{i=1}^n{y_i}$$

Taking the derivative according to $\lambda$: $$\frac{\partial l(\lambda)}{\partial\lambda}=\frac{n}{\lambda}-\sum_{i=1}^n{y_i}$$

Setting this derivative to zero: $$\frac{n}{\lambda}=\sum_{i=1}^n{y_i}$$

And finally, solving for $\lambda$ we have: $$\lambda=\frac{n}{\sum_{i=1}^n{y_i}}$$

So we arrived to the same conclusion as in the previous section: **the maximum likelihood estimation (MLE) for $\lambda$ is the reciprocal of the sample mean**.

## 3. Normal Distribution

In <a href="Chapter01.html" target="_blank">Section 2 of Chapter 1</a>, we've calculated the Body Mass Index (BMI) values of the respondents in $kg/m^2$ and used this variable as an example for the normal distribution. We omitted missing values in this variable as well in <a href="Chapter01.html" target="_blank">Chapter 1</a>, so let's do the same here too.

```{r}
ess$BMI <- ess$Weight/(ess$Height/100)^2
ess_small <- ess[!is.na(ess$BMI),c("idno", "BMI")]
hist(ess_small$BMI)
```

The histogram nicely shows the bell curve of the normal density function: most people in this dataset have a BMI around 27, and around this mode of 27, we have smaller and larger BMIs with proportionally decreasing frequencies. Extremely small and large values have only a few observations.

Parameters to estimate for the normal distribution are $\mu$ and $\sigma$, so in this case: $\theta=(\mu,\sigma)$

Let's write the negative log-likelihood function for our sample in case of the normal distribution as a custom R function. The input parameter `theta_norm` is treated as a `vector` since it contains two parameters: $\mu,\sigma$.

```{r}
# function definition
neg_log_likelihood_norm <- function(theta_norm){
  log_lik_observations <- log(dnorm(x = ess_small$BMI,
                                    mean = theta_norm[1],
                                    sd = theta_norm[2])) # log-probability of every observations occurring under the current parameters
  return(-sum(log_lik_observations))
}

# function test on some random parameter combination
neg_log_likelihood_norm(c(250, 250))
```

Do the optimization with `optim` like for the Exponential distribution. The first parameter of `optim` here are the starting $\mu$ and $\sigma$ values for the optimization given in `vector` format. These values can be any arbitrary starting values We can *ignore the warnings* like before.

```{r warning=FALSE}
norm_ml <- optim(c(250, 250), neg_log_likelihood_norm)
norm_ml$par # optimal parameter values
norm_ml$val # minimized negative log-likelihood of the sample
```

So, our results are $\hat{\mu}_{ML}=26.27$ and $\hat{\sigma}_{ML}=4.028872$.

See that the $\hat{\sigma}_{ML}$ is actually the **uncorrected sample standard deviation** (where we divide by $n$ and not $n-1$ like the `sd` function). Difference is just some rounding error.

```{r}
sqrt(mean((ess_small$BMI - mean(ess_small$BMI ))^2))
```

So, this is **NOT the corrected sample standard deviation**.

```{r}
sd(ess_small$BMI)
```

This is a rather interesting result. We should take note of it now, and the implications are to be discussed in <a href="Chapter04.html" target="_blank">Chapter 4</a>.

### 3.1. Analitical Solution for the Normal Distribution

We can derive the results for the two parameters of the Normal distribution we've seen in *Section 3.* analytically as well. Just like we did for the Exponential distribution in *Section 2.1.*<br>
So, let's **take the derivative of the log-likelihood** according to **both $\theta$ parameters** and solve the equations of **these two derivatives being equal to $0$:** $$\frac{\partial l(\theta)}{\partial\theta_i}=0$$

Where $\theta=(\mu, \sigma)$, so $\theta_1=\mu$ and $\theta_2=\sigma$.

The $P(y_i|\theta)=f(y_i,\theta)$ probabilities for the Exponential distribution (where $\theta=(\mu, \sigma)$) are given by the Normal density function: $$f(y_i, \mu, \sigma)=\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i-\mu)^2}{2\sigma^2}\right)$$

The likelihood of the whole sample is the individual $y_i$ likelihoods multiplied together because of the $y_i$s independence (IID sample): $$L(\mu, \sigma)=\prod_{i=1}^n{\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i-\mu)^2}{2\sigma^2}\right)}$$

We can expand the product by using the fact that multiplication becomes additive in the power: $$L(\mu, \sigma)=\left(2\pi\sigma^2\right)^{-n/2} \exp\left(-\sum_{i=1}^n{\frac{(y_i-\mu)^2}{2\sigma^2}}\right)$$

Taking the natural logarithm of the likelihood function to produce the log-likelihood: $$l(\mu, \sigma)=\ln(L(\mu, \sigma))=\ln\left(\left(2\pi\sigma^2\right)^{-n/2} \exp\left(-\sum_{i=1}^n{\frac{(y_i-\mu)^2}{2\sigma^2}}\right)\right)$$

Using logarithm properties we can make the expression additive, so its easier to derivative: $$l(\mu, \sigma)=\frac{n}{2} \ln(2\pi\sigma^2)-\sum_{i=1}^n{\frac{(y_i-\mu)^2}{2\sigma^2}}$$

Taking the derivative with respect to $\mu$ (remember $\sigma$ is treated as a constant term here): $$\frac{\partial l(\mu, \sigma)}{\partial\mu}=\sum_{i=1}^n{\frac{y_i-\mu}{\sigma^2}}$$

We've used the chain rule to get the derivative of the $(y_i-\mu)^2$ part which is $2(y_i-\mu) \times (-1)$. We can simplify with $2$ in the division. And the $-1$ takes care of the $-$ before the sum.

So, setting this derivative to zero, we have: $$0=\sum_{i=1}^n{(y_i-\mu)}$$

And finally, solving for $\mu$ we get that: $$\mu=\frac{\sum_{i=1}^n{y_i}}{n}=\bar{y}$$

Notice that we actually we just needed to maximize the $-\sum_{i=1}^n{(y_i-\mu)^2}$ part which is equivalent with minimizing $\sum_{i=1}^n{(y_i-\mu)^2}$ according to $\mu$. So, we can **get the maximum likelihood estimation for the Normal distribution's expected value by minimizing the squared error**. This is the **ordinary least squares** logic learnt on the *Fundamentals of Statistics* course!! That is, the **mean is the statistical measure that minimizes the standard deviation, i.e. it has minimal squared error when we substitute all observations with the mean**!

Now, let's see the **maximum likelihood estimation of $\sigma$:** we need to take the derivative of the log-likelihood with respect to $\sigma$ (remember $\mu$ is treated as a constant term here): $$\frac{\partial l(\mu, \sigma)}{\partial\sigma^2}=-\frac{n}{2\sigma^2}+\sum_{i=1}^n{\frac{(y_i-\mu)^2}{2\sigma^4}}$$

Setting this derivative to zero, we have: $$\frac{n}{2\sigma^2}=\sum_{i=1}^n{\frac{(y_i-\mu)^2}{2\sigma^4}}$$

Multiplying both sides by $2\sigma^4$, we get: $$n \times \sigma^2=\sum_{i=1}^n{(y_i-\mu)^2}$$

Solving for $\sigma^2$: $$\sigma^2=\frac{\sum_{i=1}^n{(y_i-\mu)^2}}{n}$$

So, we've arrived at the same conclusion as in *Section 3* with the numerical solution: the maximum likelihood estimation for the variance of a Normal distribution is the **uncorrected sample standard deviation**, where we divide by $n$ and NOT by $n-1$.

## 4. Negative Binomial Distribution

We've used the *CarInsurance_NumberOfClaims_v2* sheet of the <a href="https://github.com/KoLa992/Computational-Statistics-Lecture-Notes/blob/main/insurance_examples.xlsx" target="_blank">insurance_examples.xlsx</a> file as an example for the Negative Binomial distribution in <a href="Chapter01.html" target="_blank">Section 5.4 of Chapter 1</a>. Here, we have $n=400$ observation of insured cars and the `NumClaims` variable contains the number of accidents (or claims) reported in a year. In <a href="Chapter01.html" target="_blank">Chapter 1</a>, we discussed that here we have young drivers who are quite risky, so their claim number is Poisson with a quite high $\lambda_1$ average claim number (Remember that if $Y \sim P(\lambda_1)$, then $E(Y)=\lambda_1$). Then we have middle-aged drivers with experience, so their claim number also follows a Poisson distribution, but with the lowest $\lambda_2$ average claim number. For elderly drivers the claim number still has a Poisson distribution but with a $\lambda_3$ average claim number that is in between $\lambda_1$ and $\lambda_2$ as elderly drivers are riskier for their slower reflexes but it is compensated with their experience. So, if we have these 3 driver groups and we are examining all 3 combined in a variable containing the number of claims without knowing who belongs to which group, then this combined variable follows a Negative Binomial distribution.

```{r}
num_claims <- readxl::read_excel("insurance_examples.xlsx", sheet = "CarInsurance_NumberOfClaims_v2")
str(num_claims)
```

We can confirm that a discrete variable follows a Negative Binomial distribution by checking whether its variance is bigger than its mean.

```{r}
sd(num_claims$NumClaims)^2 > mean(num_claims$NumClaims)
```

Great, we again have that $s^2>\bar{y}$, so we can go ahead with fitting a Negative Binomial distribution. :)

The parameters to estimate for the Binomial distribution are $r$ and $p$, so in this case: $\theta=(r,p)$

Now, let's write the negative log-likelihood function for our sample as a custom R function in this case as well.<br>
The input parameter of the function, `theta_nbinom` is treated as a `vector` where the 1st value is the $r$ parameter and the 2nd value is the $p$ parameter. We can use the `dnbinom` function for calculating the $P(y_i|\theta)=f(y_i,\theta)$ probabilities for the Negative Binomial distribution. Then, we just apply the $l(\theta)=\sum_{i=1}^{n}{\ln(f(y_i,\theta)})$ formula.

```{r}
# function definition
neg_log_likelihood_nbinom <- function(theta_nbinom){
  log_lik_observations <- log(dnbinom(x = num_claims$NumClaims,
                                     size = theta_nbinom[1],
                                     prob = theta_nbinom[2])) # log-probability of every observations occurring under the current parameters
  return(-sum(log_lik_observations))
}

# function test on some random parameter combination
neg_log_likelihood_nbinom(c(8, 0.1))
```

Do the **optimization with R's built-in `optim` function**. The first parameter of this function are the starting $r$ and $p$ values for the optimization given in `vector` format. These values can be any arbitrary starting values, but we should respect the constraints on the Negative Binomial parameters: like $p$ should be between $0$ and $1$. We can *ignore the possible warnings*, it just means that R encountered some bad parameter combinations when trying out different $r$ and $p$ values during the optimization. R automatically ignores these cases, they won't cause any problems.

```{r warning=FALSE}
nbinom_ml <- optim(c(8, 0.1), neg_log_likelihood_nbinom)
nbinom_ml$par # optimal parameter values
nbinom_ml$val # minimized negative log-likelihood of the sample
```

So, our results are $\hat{r}_{ML}=1.7$ and $\hat{p}_{ML}=0.52$.

## 5. Compare Different Distributions Fitted on the Same Data - Information Criterions

In <a href="Chapter01.html" target="_blank">Section 4.2 of Chapter 1</a>, we used the daily internet usage time in minutes variable in the ESS data frame as an example for a lognormal distribution. We removed all missing values just like for the alcohol consumption and BMI, so let's do that here as well.

```{r}
ess_small <- ess[!is.na(ess$DailyNetUse),c("idno", "DailyNetUse")]
```

Furthermore, let's remove the $0$ values as well from the variable as a proper lognormal distribution can only take positive values. It only means ommitting $6$ out of $1397$ observations, so it's not a big loss.

```{r}
sum(ess_small$DailyNetUse == 0)
ess_small <- ess_small[ess_small$DailyNetUse > 0,]
hist(ess_small$DailyNetUse)
```

Based on the histogram, we discussed in <a href="Chapter01.html" target="_blank">Section 4.2 of Chapter 1</a> that the shape we can see here looks like the lognormal density function: because we have a long right tail here as well just like in the case of alcohol consumption, but the most frequent bin is not the first one (like it was on the same histogram for alcohol consumption), but the second one: the 100-200 minutes a day bin. The first bin of 0-100 minutes is only the second most frequent one.

Okay, so let's fit the lognormal distribution with maximum likelihood and hoperfully it won't produce the same bad fit like the MM when using the 1st and 2nd moments.

Parameters to estimate for the lognormal distribution are $\mu$ and $\sigma$ on the log-scale, so in this case: $\theta=(\mu,\sigma)$

Let's write the negative log-likelihood function for our sample in case of the lognormal distribution as a custom R function. The input parameter `theta_lnorm` is treated as a `vector` just like for the normal distribution in Section 3.

```{r}
# function definition
neg_log_likelihood_lnorm <- function(theta_lnorm){
  log_lik_observations <- log(dlnorm(x = ess_small$DailyNetUse,
                                     meanlog = theta_lnorm[1],
                                     sdlog = theta_lnorm[2])) # log-probability of every observations occurring under the current parameters
  return(-sum(log_lik_observations))
}

# function test on some random parameter combination
neg_log_likelihood_lnorm(c(2, 2))
```

Do the optimization. We can *ignore the warnings* like before.

```{r warning=FALSE}
lnorm_ml <- optim(c(2,2), neg_log_likelihood_lnorm)
lnorm_ml$par # optimal parameter values
lnorm_ml$val # minimized negative log-likelihood of the sample
```

We can check on a histogram with `ggplot` that our fitted lognormal density function with maximum likelihood parameters fits on the observed histogram and does not suffer like our solution with clasical MM.

```{r}
library(ggplot2)
ggplot(data = ess_small, mapping = aes(x=DailyNetUse)) + geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dlnorm, 
                args = list(meanlog = lnorm_ml$par[1], sdlog = lnorm_ml$par[2]),
                col = 'blue',
                linewidth = 1)
```

Great, now we can see that the estimation based on the ML method fits well on the observed histogram for the first try since it utilizes every observation in the observed sample when estimating the $\mu,\sigma$ parameters of the lognormal distribution.

However, we can also **try fitting an exponential distribution** on the observed daily net usage time as well. In this case we **assume** that the decrease in frequencies in the first few bins is just sampling error and the **long right tail of the observed distribution is described better for unobserved data by the Exponential distribution**. Then, we'll try to find an **objective statistical measure to decide whether the exponential or the lognormal distribution describes the long right tail of our distribution better**.

So, let's fit exponential with MLE with its one and only $\theta=\lambda$ parameter.In the following code, everything works the same as in Section 2. We just switch the variable in the first parameter of the `dexp`function. If we were more civilized people, we would have wrote our `neg_log_likelihood_expon` function to take the observed sample data in `vector` format as an input parameter, but this <a href="https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fabsolutely-barbaric-v0-w3y1u19lfnqc1.png%3Fwidth%3D640%26crop%3Dsmart%26auto%3Dwebp%26s%3D6bf749538e0ea75c93f799686d985e263ea5403c" target="_blank">absolutely barbaric</a> solution will serve our purposes just fine as well.

```{r}
# function definition
neg_log_likelihood_expon <- function(theta_expon){
  log_lik_observations <- log(dexp(x = ess_small$DailyNetUse,
                                   rate = theta_expon))
  return(-sum(log_lik_observations))
}

# function test on some random parameter value
neg_log_likelihood_expon(0.001)
```

Do the **optimization with R's built-in `optim` function**. Exactly like before for every distribution.

```{r warning=FALSE}
expon_ml <- optim(0.001, neg_log_likelihood_expon)
expon_ml$par # optimal parameter value
expon_ml$val # minimized negative log-likelihood of the sample
```

Great, we have that for the daily net usage time data, a $\hat{\lambda}_{ML}=0.0054$ is best if an exponential distribution is assumed according to the maximum likelihood method.

Now, let's compare the **lognormal and exponential distributions with the histogram of the daily net usage time data**. Graphically and logically, its obvious that the lognormal is a better fit because of the decrease of frequencies in the first few bins and the shraper decreease in frequencies for the outlier values in the right tail of the distribution.<br>
We apply the `ggplot` package here: we add two `stat_function` layers to the `geom_histogram`: one for the Lognormal (*blue*) and another for the Exponential (*red*) density functions.

```{r}
ggplot(data = ess_small, mapping = aes(x=DailyNetUse)) + geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dlnorm, 
                args = list(meanlog = lnorm_ml$par[1], sdlog = lnorm_ml$par[2]),
                col = 'blue',
                linewidth = 1) +
  stat_function(fun = dexp, 
                args = list(rate = expon_ml$par),
                col = 'red',
                linewidth = 1)
  
```

We can see that the lognormal is a better fit on the observed histogram than the exponential form the results of the maximum likelihood estimation as well. It can be concluded that **the Lognormal distribution fits better on the observed data by having smaller minimized negative log-likelihood value**, i.e.: smaller "*error*" at the end of the optimization process. Or we can say that **the distribution with higher maximized log-likelihood has a greater probability to generate our observed data in an IID sampling** of $1391$ observations.

```{r}
expon_ml$value > lnorm_ml$value
```

However, it is NOT always enough to say that if one distribution has a better maximized log-likelihood value, it is a better fit for the data. We need to **control for the fact that the lognormal distribution is more flexible as it has two parameters instead of one**. So, what we need to consider is a phenomenon called **overfitting**. Overfitting means that **a distribution only fits our data better because it has more parameters, so its density/mass function is more flexible** and more of its properties can be customized. Remember from <a href="Chapter01.html" target="_blank">Chapter 1</a>: the parameter $\mu$ determines the maximum point of the lognormal density function and the $\sigma$ parameter determines its kurtosis and the sharpness of its long right tail. While for the Exponential distribution, we only have $\lambda$ and it only influences its kurtosis  This means that **a distribution with more parameters has more chances to fit on the noise in the observed histogram and NOT on its main tendencies**. This is problematic, as we want to use the fitted distribution for predicting the statistical properties (e.g. percentiles) of unobserved data. And it **causes uncomfortable situations if we predict the noise of the data, not its main tendencies**. This phenomenon is called **overfitting** and we need to **control for this when selecting the best fitting distribution** for our data. So right now, we need to **make sure that the lognormal is not a better fit than the exponential just because it has more parameters**.

Our **tools for controlling overfitting are the Information Criterion** ($IC$s). All the $IC$s are **based on the negative log-likelihood, so they always needs to be minimized**. This means that the smaller $IC$ shows a better distribution fit. However, every $IC$ **applies a penalty on the negative log-likelihood that is dependent on the number of estimated parameters** ($p$). The specific $IC$s differ on **how strong this penalty should be**. The logic is that a **distribution with more parameters is more likely to have a smaller negative log-likelihood** However, when the **penalty term** of the $IC$s are **applied**, then they need to **cause more negative log-likelihood decrease in order to get preferred than a distribution with fewer parameters**. Because the distribution with more parameter is more likely to fit on the noise in the data and not on its main tendencies. So, the $IC$s **level the playing field** between distributions with different number of parameters so to speak.

We cover two specific $IC$s now.

The Akaike Information Criterion ($AIC$) is the one with the **most lenient penalty on the number of parameters** among all the $IC$s. It has many specific formulas, but the simplest one is the following with $p$ being the number of estimated parameters: $$AIC=2p-2l(\theta)$$ 

It's quite easy to calculate in R.

```{r}
aic_exp <- 2*1 + 2*expon_ml$value # as the exponential distribution has p=1 parameter
aic_lnorm <- 2*2 + 2*lnorm_ml$value # as the lognormal distribution has p=2 parameters
```

The Bayes - Schwarz Information Criterion ($BIC$ or $SBC$) has the **strictest penalty on the number of parameters** among all the $IC$s. It has many specific formulas, but the simplest one is the following: $$BIC=p\ln(n) - 2l(\theta)$$

The $BIC$ can be implemented in R as follows.

```{r}
bic_exp <- 1*log(nrow(ess_small)) + 2*expon_ml$value # as the exponential distribution has p=1 parameter
bic_lnorm  <- 2*log(nrow(ess_small)) + 2*lnorm_ml$value # as the normal distribution has p=2 parameters
```

When calculating these two values, $AIC$ and $BIC$ show that the lognormal is truly a better fit, despite having two parameter instead of one.

```{r}
aic_exp > aic_lnorm
bic_exp > bic_lnorm
```

As the $BIC$ has a stronger penalty on $p$ in its formula, so we can state that for all distributions: $AIC < BIC$

```{r}
aic_lnorm < bic_lnorm
aic_exp < bic_exp
```

It follows that **$BIC$ usually always prefers distributions with fewer parameters, than the $AIC$**. There are many more $IC$s in between the values of $AIC$ and $BIC$, we'll cover some later in this course.

As I've briefly mentioned, **$IC$s have several specific formulas and R doesn't necessarily use the ones I just showed You**. It **does not matter which specific formulas You use as long as You apply them consistently**: like such that the $AIC<BIC$ relationship remains.